{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/144 [00:00<?, ?day/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not retrieve data for date 2020-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 144/144 [00:11<00:00, 12.07day/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 11.950767755508423 seconds\n"
     ]
    }
   ],
   "source": [
    "base_url = \"http://agbc-fe.pdn.ac.lk/api/v1/data/?sensor=10008&date=\"\n",
    "\n",
    "start_date = pd.to_datetime(\"2020-10-22\")\n",
    "end_date = pd.to_datetime(\"2021-03-14\")\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "def fetch_data(date):\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    url = base_url + date_str\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        return data['data']\n",
    "    except:\n",
    "        print(f\"Error: Could not retrieve data for date {date_str}\")\n",
    "        return []\n",
    "\n",
    "start_time = time.time()  # Get the current time before starting the execution\n",
    "\n",
    "\n",
    "# Create a ThreadPoolExecutor with the maximum number of workers\n",
    "executor = ThreadPoolExecutor(max_workers=None)\n",
    "\n",
    "# Use tqdm to track the progress\n",
    "with tqdm(total=len(date_range), desc=\"Progress\", unit=\"day\") as pbar:\n",
    "    # Submit the fetch_data task to the executor for each date in parallel\n",
    "    futures = [executor.submit(fetch_data, date) for date in date_range]\n",
    "    \n",
    "    # Retrieve the results from the completed futures\n",
    "    for future in futures:\n",
    "        all_data.extend(future.result())\n",
    "        pbar.update(1)\n",
    "    \n",
    "\n",
    "end_time = time.time()  # Get the current time after finishing the execution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(\"Execution Time:\", execution_time, \"seconds\")\n",
    "\n",
    "# Create the DataFrame from the collected data\n",
    "df = pd.DataFrame(all_data, dtype=str)\n",
    "df.to_csv('dataws.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXMbj9kvrn1H",
    "outputId": "56c43792-dfe6-4cc4-9654-1b01d336140f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siteId       0\n",
      "seqNo        0\n",
      "date         0\n",
      "time         0\n",
      "temp1        0\n",
      "temp2        0\n",
      "temp3        0\n",
      "humidity1    0\n",
      "humidity2    0\n",
      "humidity3    0\n",
      "light        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zBhqM74Brs51"
   },
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "joYskXyPrzA8"
   },
   "outputs": [],
   "source": [
    "# Drop duplicate rows\n",
    "df=df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gEd3DbvksApo",
    "outputId": "a1a4440a-e2ab-4d00-9060-73107b549e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       siteId  seqNo         date       time  temp1  temp2  temp3 humidity1  \\\n",
      "340154      0   2869   2021-03-14   23:55:03    NaN   26.1   24.9       NaN   \n",
      "340155      0   2870   2021-03-14   23:55:31     25   26.2   24.9      79.6   \n",
      "340156      0   2871   2021-03-14   23:56:01     25   26.1   24.8      79.5   \n",
      "340157      0   2872   2021-03-14   23:56:31     25   26.1   24.7      79.8   \n",
      "340158      0   2873   2021-03-14   23:57:01     25   26.1   24.8        80   \n",
      "340159      0   2874   2021-03-14   23:57:33    NaN   26.2   24.8       NaN   \n",
      "340160      0   2875   2021-03-14   23:58:03    NaN   26.1   24.8       NaN   \n",
      "340161      0   2876   2021-03-14   23:58:31   24.9   26.1   24.7      80.3   \n",
      "340162      0   2877   2021-03-14   23:59:01   24.9   26.1   24.8      80.4   \n",
      "340163      0   2878   2021-03-14   23:59:31   24.9   26.1   24.8      80.5   \n",
      "\n",
      "       humidity2 humidity3   light  \n",
      "340154        80        40   1.042  \n",
      "340155        80        40   1.042  \n",
      "340156        80        40   1.042  \n",
      "340157        80        41   1.042  \n",
      "340158        80        41   1.042  \n",
      "340159        81        41   1.042  \n",
      "340160        81        41   1.042  \n",
      "340161        81        42   1.042  \n",
      "340162        81        42   1.042  \n",
      "340163        81        42   1.042  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Replace '?' with NaN\n",
    "\n",
    "df.replace(' ?', np.nan, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print(df.tail(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a new DataFrame with Average Temperature and Average Humidity Values </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3DnDwFzcrdM",
    "outputId": "93ea969a-939d-4c10-b47b-35828d743a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   seqNo         date       time  average_internal_temp  \\\n",
      "0      1   2020-10-22   00:00:01              23.433333   \n",
      "1      2   2020-10-22   00:00:18              23.550000   \n",
      "2      3   2020-10-22   00:00:31              23.433333   \n",
      "3      4   2020-10-22   00:00:46              23.466667   \n",
      "4      5   2020-10-22   00:01:01              23.400000   \n",
      "\n",
      "   average_internal_humidity   light  \n",
      "0                  96.300000   1.042  \n",
      "1                  95.000000   1.042  \n",
      "2                  96.266667   1.042  \n",
      "3                  96.200000   1.042  \n",
      "4                  96.166667   1.042  \n"
     ]
    }
   ],
   "source": [
    "# Convert temperature columns to numeric\n",
    "df['temp1'] = pd.to_numeric(df['temp1'], errors='coerce')\n",
    "df['temp2'] = pd.to_numeric(df['temp2'], errors='coerce')\n",
    "df['temp3'] = pd.to_numeric(df['temp3'], errors='coerce')\n",
    "\n",
    "# Convert temperature columns to numeric\n",
    "df['humidity1'] = pd.to_numeric(df['humidity1'], errors='coerce')\n",
    "df['humidity2'] = pd.to_numeric(df['humidity2'], errors='coerce')\n",
    "df['humidity3'] = pd.to_numeric(df['humidity3'], errors='coerce')\n",
    "\n",
    "df['seqNo'] = pd.to_numeric(df['seqNo'], errors='coerce')\n",
    "\n",
    "# Calculate the average temperature\n",
    "df['average_internal_temp'] = df[['temp1', 'temp2', 'temp3']].mean(axis=1,skipna=True)\n",
    "\n",
    "# Calculate the average humidity\n",
    "df['average_internal_humidity'] = df[['humidity1', 'humidity2', 'humidity3']].mean(axis=1,skipna=True)\n",
    "\n",
    "# Create a new DataFrame with only the desired columns\n",
    "new_df = df[['seqNo','date','time','average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "\n",
    "\n",
    "print(new_df.head())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create a Data frame for Internal Sensor 10008 data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/5dgg62dn62g30jn00c_29n180000gn/T/ipykernel_79234/77695520.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['datetime'] = pd.to_datetime(new_df['date'] + ' ' + new_df['time'])\n",
      "/var/folders/jc/5dgg62dn62g30jn00c_29n180000gn/T/ipykernel_79234/77695520.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# Combine the 'date' and 'time' columns into a single datetime column\n",
    "new_df['datetime'] = pd.to_datetime(new_df['date'] + ' ' + new_df['time'])\n",
    "# Set the 'time' column as the DataFrame index\n",
    "new_df.set_index('datetime', inplace=True)\n",
    "new_df.drop(['date', 'time','seqNo'], axis=1, inplace=True)\n",
    "# Resample the DataFrame using 'H' offset alias and select the first entry from each hour\n",
    "new_df_hourly = new_df.resample('H').first()\n",
    "\n",
    "# new_df_hourly.reset_index(inplace=True)\n",
    "# Print the resulting DataFrame\n",
    "\n",
    "new_df_hourly.to_csv('sensor10008.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create a Data frame for External Environmental data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "external_weather = pd.read_csv('weather_data.csv')\n",
    "\n",
    "# Combine the 'Date' and 'Time' columns into a single datetime column\n",
    "external_weather['datetime'] = pd.to_datetime(external_weather['date'] + ' ' + external_weather['time'])\n",
    "\n",
    "external_weather.drop([\"time\",\"date\"],axis=1,inplace=True)\n",
    "\n",
    "external_weather.set_index('datetime', inplace=True)\n",
    "\n",
    "merged_df = pd.merge(external_weather, new_df_hourly, on='datetime')\n",
    "\n",
    "# Drop rows with any null values\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "merged_df.to_csv('data_set.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "# # Create a profile report for your DataFrame\n",
    "# profile = ProfileReport(merged_df)\n",
    "\n",
    "# # Generate the report and save it as an HTML file\n",
    "# profile.to_file('profile_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the columns for the box plot\n",
    "columns_to_plot = ['External Temperature', 'average_internal_temp','Feels Like']\n",
    "\n",
    "# Create the box plot using seaborn\n",
    "sns.boxplot(data=merged_df[columns_to_plot])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Box Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Extracting features and target variable </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like','Pressure','External Humidity','Dew Point','Clouds','Wind Speed']]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By  LinearRegression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.22630530781151162\n",
      "Test Accuracy: 0.2522579488510137\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Training the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\",test_accuracy)\n",
    "\n",
    "print(type(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Training the linear regression model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[39m=\u001b[39mRandomForestRegressor()\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Calculate training accuracy\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Splitting dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Training the linear regression model\n",
    "model =RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\",test_accuracy)\n",
    "\n",
    "print(type(X_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By  DecisionTree Regressor</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (2.0.15)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (0.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (1.21.5)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (1.11.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (22.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.3.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (5.12.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-07 21:19:49,105] A new study created in memory with name: no-name-1fc0ea65-2661-46c1-ac4f-c643188405eb\n",
      "[I 2023-06-07 21:19:49,126] Trial 0 finished with value: 0.2355344738305678 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,138] Trial 1 finished with value: 0.16897677675994752 and parameters: {'max_depth': 2, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,155] Trial 2 finished with value: 0.197824114958684 and parameters: {'max_depth': 7, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,169] Trial 3 finished with value: 0.22885115631455066 and parameters: {'max_depth': 5, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,180] Trial 4 finished with value: 0.21541983546013163 and parameters: {'max_depth': 3, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,190] Trial 5 finished with value: 0.215419835460132 and parameters: {'max_depth': 3, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,206] Trial 6 finished with value: 0.12163259338415484 and parameters: {'max_depth': 9, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,220] Trial 7 finished with value: 0.21697365304092842 and parameters: {'max_depth': 6, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,238] Trial 8 finished with value: 0.19808089039143692 and parameters: {'max_depth': 10, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,253] Trial 9 finished with value: 0.2208493252803323 and parameters: {'max_depth': 6, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,281] Trial 10 finished with value: 0.23553447383056772 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,305] Trial 11 finished with value: 0.23553447383056755 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,355] Trial 12 finished with value: 0.23553447383056755 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,399] Trial 13 finished with value: 0.17076297061112486 and parameters: {'max_depth': 7, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,429] Trial 14 finished with value: 0.16897677675994704 and parameters: {'max_depth': 2, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,465] Trial 15 finished with value: 0.23553447383056791 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,498] Trial 16 finished with value: 0.22074825073093732 and parameters: {'max_depth': 5, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,532] Trial 17 finished with value: 0.18226711485697922 and parameters: {'max_depth': 8, 'min_samples_leaf': 8}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,557] Trial 18 finished with value: 0.20556385176092284 and parameters: {'max_depth': 5, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,580] Trial 19 finished with value: 0.2154198354601321 and parameters: {'max_depth': 3, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,604] Trial 20 finished with value: 0.23553447383056772 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,636] Trial 21 finished with value: 0.23553447383056791 and parameters: {'max_depth': 4, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,661] Trial 22 finished with value: 0.20556385176092273 and parameters: {'max_depth': 5, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,683] Trial 23 finished with value: 0.21541983546013166 and parameters: {'max_depth': 3, 'min_samples_leaf': 4}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,705] Trial 24 finished with value: 0.16897677675994727 and parameters: {'max_depth': 2, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,728] Trial 25 finished with value: 0.2355344738305677 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,761] Trial 26 finished with value: 0.1811714977962925 and parameters: {'max_depth': 6, 'min_samples_leaf': 4}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,788] Trial 27 finished with value: 0.22074825073093732 and parameters: {'max_depth': 5, 'min_samples_leaf': 8}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,811] Trial 28 finished with value: 0.2154198354601317 and parameters: {'max_depth': 3, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,832] Trial 29 finished with value: 0.16897677675994757 and parameters: {'max_depth': 2, 'min_samples_leaf': 2}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,855] Trial 30 finished with value: 0.19986806637150537 and parameters: {'max_depth': 7, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,883] Trial 31 finished with value: 0.2355344738305676 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,915] Trial 32 finished with value: 0.23553447383056794 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:49,936] Trial 33 finished with value: 0.2207482507309372 and parameters: {'max_depth': 5, 'min_samples_leaf': 8}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:49,955] Trial 34 finished with value: 0.21541983546013155 and parameters: {'max_depth': 3, 'min_samples_leaf': 5}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:49,987] Trial 35 finished with value: 0.21138619188955834 and parameters: {'max_depth': 6, 'min_samples_leaf': 8}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,013] Trial 36 finished with value: 0.23553447383056783 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,038] Trial 37 finished with value: 0.21541983546013163 and parameters: {'max_depth': 3, 'min_samples_leaf': 10}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,071] Trial 38 finished with value: 0.2041795564164013 and parameters: {'max_depth': 5, 'min_samples_leaf': 3}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,102] Trial 39 finished with value: 0.23553447383056766 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,137] Trial 40 finished with value: 0.16897677675994713 and parameters: {'max_depth': 2, 'min_samples_leaf': 4}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,182] Trial 41 finished with value: 0.23553447383056758 and parameters: {'max_depth': 4, 'min_samples_leaf': 10}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,218] Trial 42 finished with value: 0.23089325442235384 and parameters: {'max_depth': 5, 'min_samples_leaf': 9}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,244] Trial 43 finished with value: 0.21541983546013208 and parameters: {'max_depth': 3, 'min_samples_leaf': 8}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,267] Trial 44 finished with value: 0.2355344738305678 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,292] Trial 45 finished with value: 0.23553447383056744 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,319] Trial 46 finished with value: 0.19808089039143698 and parameters: {'max_depth': 10, 'min_samples_leaf': 9}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,343] Trial 47 finished with value: 0.21138619188955834 and parameters: {'max_depth': 6, 'min_samples_leaf': 8}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,365] Trial 48 finished with value: 0.17076297061112497 and parameters: {'max_depth': 7, 'min_samples_leaf': 6}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,392] Trial 49 finished with value: 0.21541983546013202 and parameters: {'max_depth': 3, 'min_samples_leaf': 7}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,452] Trial 50 finished with value: 0.22885115631455055 and parameters: {'max_depth': 5, 'min_samples_leaf': 10}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,491] Trial 51 finished with value: 0.23553447383056805 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,518] Trial 52 finished with value: 0.2355344738305676 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,551] Trial 53 finished with value: 0.21541983546013202 and parameters: {'max_depth': 3, 'min_samples_leaf': 5}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,584] Trial 54 finished with value: 0.23553447383056783 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,621] Trial 55 finished with value: 0.2355344738305675 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,663] Trial 56 finished with value: 0.21202567045429724 and parameters: {'max_depth': 5, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,699] Trial 57 finished with value: 0.21541983546013221 and parameters: {'max_depth': 3, 'min_samples_leaf': 5}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,726] Trial 58 finished with value: 0.2355344738305679 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,750] Trial 59 finished with value: 0.2207482507309374 and parameters: {'max_depth': 5, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,778] Trial 60 finished with value: 0.1547803987325391 and parameters: {'max_depth': 8, 'min_samples_leaf': 5}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,804] Trial 61 finished with value: 0.2355344738305677 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,832] Trial 62 finished with value: 0.23553447383056783 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,858] Trial 63 finished with value: 0.23553447383056755 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,881] Trial 64 finished with value: 0.23553447383056794 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,907] Trial 65 finished with value: 0.2154198354601318 and parameters: {'max_depth': 3, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,937] Trial 66 finished with value: 0.22074825073093732 and parameters: {'max_depth': 5, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,965] Trial 67 finished with value: 0.23553447383056758 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,992] Trial 68 finished with value: 0.2154198354601318 and parameters: {'max_depth': 3, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,023] Trial 69 finished with value: 0.20417955641640126 and parameters: {'max_depth': 5, 'min_samples_leaf': 4}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,063] Trial 70 finished with value: 0.211253550013766 and parameters: {'max_depth': 6, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,096] Trial 71 finished with value: 0.23553447383056783 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,146] Trial 72 finished with value: 0.2355344738305676 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,181] Trial 73 finished with value: 0.2355344738305675 and parameters: {'max_depth': 4, 'min_samples_leaf': 5}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,206] Trial 74 finished with value: 0.21541983546013208 and parameters: {'max_depth': 3, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,233] Trial 75 finished with value: 0.23553447383056747 and parameters: {'max_depth': 4, 'min_samples_leaf': 5}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,258] Trial 76 finished with value: 0.2207482507309373 and parameters: {'max_depth': 5, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,282] Trial 77 finished with value: 0.215419835460132 and parameters: {'max_depth': 3, 'min_samples_leaf': 3}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,304] Trial 78 finished with value: 0.16897677675994718 and parameters: {'max_depth': 2, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,330] Trial 79 finished with value: 0.23553447383056778 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,363] Trial 80 finished with value: 0.2120256704542972 and parameters: {'max_depth': 5, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,390] Trial 81 finished with value: 0.23553447383056755 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,415] Trial 82 finished with value: 0.2355344738305678 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,445] Trial 83 finished with value: 0.23553447383056755 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,480] Trial 84 finished with value: 0.23553447383056725 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,521] Trial 85 finished with value: 0.22074825073093726 and parameters: {'max_depth': 5, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,561] Trial 86 finished with value: 0.23553447383056772 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,602] Trial 87 finished with value: 0.21541983546013174 and parameters: {'max_depth': 3, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,654] Trial 88 finished with value: 0.23553447383056805 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,700] Trial 89 finished with value: 0.22074825073093737 and parameters: {'max_depth': 5, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,745] Trial 90 finished with value: 0.20675165419287878 and parameters: {'max_depth': 9, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,770] Trial 91 finished with value: 0.2355344738305679 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,797] Trial 92 finished with value: 0.2355344738305676 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,821] Trial 93 finished with value: 0.2355344738305679 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,846] Trial 94 finished with value: 0.2355344738305677 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,871] Trial 95 finished with value: 0.21541983546013174 and parameters: {'max_depth': 3, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,897] Trial 96 finished with value: 0.22885115631455033 and parameters: {'max_depth': 5, 'min_samples_leaf': 10}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,925] Trial 97 finished with value: 0.215419835460132 and parameters: {'max_depth': 3, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,964] Trial 98 finished with value: 0.2355344738305677 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,999] Trial 99 finished with value: 0.23553447383056766 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Max Depth is :  4\n",
      "Best Min Samples Leaf is :  7\n",
      "Training Accuracy: 0.365247381232352\n",
      "Test Accuracy: 0.23553447383056728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "# Assuming you have your input features in X and output features in y\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10)\n",
    "    \n",
    "    # Create the decision tree regressor object with the suggested parameters\n",
    "    clf = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate the test accuracy\n",
    "    test_accuracy = clf.score(X_test, y_test)\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best parameters from the study\n",
    "best_params = study.best_params\n",
    "\n",
    "\n",
    "clf = DecisionTreeRegressor(max_depth=best_params['max_depth'], min_samples_leaf=best_params['min_samples_leaf'])\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best Max Depth is : \",best_params['max_depth'])\n",
    "\n",
    "print(\"Best Min Samples Leaf is : \",best_params['min_samples_leaf'])\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By Lasso</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9205226.375951892\n",
      "Score: 0.21910478812166045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have a pandas DataFrame 'data' containing your feature columns (X) and target column (y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Create the Lasso regression model\n",
    "lasso = Lasso(alpha=0.001)  # Adjust the alpha parameter to control the degree of regularization\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "# Get the score (coefficient of determination) on the testing data\n",
    "score = lasso.score(X_test, y_test)\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Trained By Ridge regression </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9199522.69808088\n",
      "R2 Score: 0.21962995795551687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have your feature matrix X and target variable y\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (optional but recommended for regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a Ridge regression model\n",
    "ridge = Ridge(alpha=1.0)  # You can adjust the regularization strength by changing the alpha parameter\n",
    "\n",
    "# Train the model\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MLPRegressor</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -0.0011481089061574634\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have your feature matrix X and target variable y\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like','Pressure','External Humidity','Dew Point','Clouds','Wind Speed']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#\n",
    "# Create the model\n",
    "model = MLPRegressor(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target values for the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.score(X_test, y_test)\n",
    "print('Score:', score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RANDOM FOREST REGRESSOR</h1> 39% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4105804367043759\n",
      "Training Accuracy: 0.7917583974727208\n",
      "Test Accuracy: 0.4105804367043759\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have your feature matrix X and target variable y\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like','Pressure','External Humidity','Dew Point','Clouds','Wind Speed']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#\n",
    "# Create the model\n",
    "model =RandomForestRegressor(n_estimators=150, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target values for the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.score(X_test, y_test)\n",
    "print('Score:', score)\n",
    "\n",
    "\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp37-cp37m-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp37-cp37m-win_amd64.whl (266.3 MB)\n",
      "     -------------------------------------- 266.3/266.3 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.2-cp37-cp37m-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     -------------------------------------- 439.2/439.2 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 4.3 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 4.1 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ---------------------------------------- 126.5/126.5 kB ? eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     ---------------------------------------- 24.4/24.4 MB 4.2 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-win_amd64.whl (896 kB)\n",
      "     -------------------------------------- 896.6/896.6 kB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.6.3)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.9/93.9 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.19.1-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.3/181.3 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.3/181.3 kB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.14)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.11.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.9/83.9 kB ? eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.19.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.54.2 h5py-3.8.0 keras-2.11.0 libclang-16.0.0 markdown-3.4.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4628\\979264105.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# **Simple neural network**\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0.post5-py3-none-any.whl size=2970 sha256=7b142af98f9f9fa06f67c4ea6a8f3a720e69f40256ec2216bc0d5c3d1d7dfec0\n",
      "  Stored in directory: c:\\users\\vilak\\appdata\\local\\pip\\cache\\wheels\\b2\\af\\1b\\ac28f3fb36a8428e3089acdd913e9ee1808e781e3ff6ce2929\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post5\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post5-py3-none-any.whl\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post5\n"
     ]
    }
   ],
   "source": [
    "# **Simple neural network**\n",
    "\n",
    "import tensorflow as tf\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
    "\n",
    "# Convert the X_train and y_train variables to type float32\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "X_train= tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train= tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "# **Deep neural network**\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Convert the X_train and y_train variables to type float32\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/hari/anaconda3/lib/python3.10/site-packages (2.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 12632283.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 2/10, Loss: 11679390.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 3/10, Loss: 11610776.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 4/10, Loss: 11662118.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 5/10, Loss: 11680528.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 6/10, Loss: 11602964.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 7/10, Loss: 11607462.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 8/10, Loss: 11577952.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 9/10, Loss: 11553023.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 10/10, Loss: 11655988.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "18/18 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Load the input and output data from the pandas DataFrame\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like', 'Pressure', 'External Humidity', 'Dew Point', 'Clouds', 'Wind Speed']]\n",
    "\n",
    "# Convert the pandas DataFrame to numpy arrays with float32 data type\n",
    "X = X.astype('float32').to_numpy()\n",
    "y = y.astype('float32').to_numpy()\n",
    "\n",
    "# Define the model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(6,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(3))  # Output layer with 3 units for the 3 output columns\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_y, test_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and compute the loss\n",
    "    history = model.fit(train_X, train_y, batch_size=32, epochs=1, verbose=0)\n",
    "\n",
    "    # Compute training and testing accuracy\n",
    "    train_loss, train_accuracy = model.evaluate(train_X, train_y, verbose=0)\n",
    "    test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=0)\n",
    "\n",
    "    # Print the loss and accuracy after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {history.history['loss'][0]}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Evaluation\n",
    "predictions = model.predict(test_X)\n",
    "# Perform further evaluation or analysis on the predictions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Architecture similar to VGG net</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "72/72 [==============================] - 3s 30ms/step - loss: 12835996.0000 - accuracy: 0.4499 - val_loss: 28828062.0000 - val_accuracy: 0.4903\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 1s 19ms/step - loss: 11914878.0000 - accuracy: 0.4490 - val_loss: 27798042.0000 - val_accuracy: 0.4903\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 12081573.0000 - accuracy: 0.4490 - val_loss: 28260008.0000 - val_accuracy: 0.4903\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 11768184.0000 - accuracy: 0.4490 - val_loss: 28005606.0000 - val_accuracy: 0.4903\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 11943424.0000 - accuracy: 0.4490 - val_loss: 30081374.0000 - val_accuracy: 0.4903\n",
      "Epoch 6/300\n",
      "58/72 [=======================>......] - ETA: 0s - loss: 11965385.0000 - accuracy: 0.4510"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the input and output data from the pandas DataFrame\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like', 'Pressure', 'External Humidity', 'Dew Point', 'Clouds', 'Wind Speed']]\n",
    "\n",
    "# Convert the pandas DataFrame to numpy arrays with float32 data type\n",
    "X = X.astype('float32').to_numpy()\n",
    "y = y.astype('float32').to_numpy()\n",
    "\n",
    "# Reshape the input data to match the VGG16 input shape\n",
    "X = X.reshape(-1, 6, 1, 1)\n",
    "\n",
    "# Define the VGG-like model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(6, 1, 1)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(3))  # Output layer with 3 units for the 3 output columns\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_y, test_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 300\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Forward pass and compute the loss\n",
    "#     history = model.fit(train_X, train_y, batch_size=32, epochs=1, verbose=0)\n",
    "\n",
    "#     # Compute training and testing accuracy\n",
    "#     train_loss, train_accuracy = model.evaluate(train_X, train_y, verbose=0)\n",
    "#     test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=0)\n",
    "\n",
    "#     # Print the loss and accuracy after each epoch\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {history.history['loss'][0]}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# # Evaluation\n",
    "# predictions = model.predict(test_X)\n",
    "# Perform further evaluation or analysis on the predictions\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=10, mode='max')\n",
    "checkpoint_callback = ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "\n",
    "history = model.fit(train_X, train_y, batch_size=32, epochs=num_epochs, validation_data=(test_X, test_y),\n",
    "                    callbacks=[early_stopping_callback, checkpoint_callback])\n",
    "\n",
    "# Load the best model saved during training\n",
    "model = models.load_model('model.h5')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=0)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# num_epochs = 400\n",
    "\n",
    "# history = model.fit(train_X, train_y, batch_size=32, epochs=num_epochs, validation_data=(test_X, test_y), verbose=1)\n",
    "\n",
    "# Retrieve accuracy values from history\n",
    "train_accuracy = history.history['accuracy']\n",
    "test_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracy, label='Train Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), test_accuracy, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LSTM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 15612470.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 2/100, Loss: 14598738.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 3/100, Loss: 12457932.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 4/100, Loss: 11612101.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 5/100, Loss: 11579144.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 6/100, Loss: 11580081.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 7/100, Loss: 11579932.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 8/100, Loss: 11586812.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 9/100, Loss: 11577862.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 10/100, Loss: 11584354.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 11/100, Loss: 11578469.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 12/100, Loss: 11576841.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 13/100, Loss: 11581021.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 14/100, Loss: 11577624.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 15/100, Loss: 11580880.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 16/100, Loss: 11578029.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 17/100, Loss: 11577312.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 18/100, Loss: 11578397.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 19/100, Loss: 11577443.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 20/100, Loss: 11578469.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 21/100, Loss: 11576154.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 22/100, Loss: 11577584.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 23/100, Loss: 11575731.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 24/100, Loss: 11579461.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 25/100, Loss: 11580708.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 26/100, Loss: 11576082.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 27/100, Loss: 11580777.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 28/100, Loss: 11577246.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 29/100, Loss: 11579814.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 30/100, Loss: 11576832.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 31/100, Loss: 11580394.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 32/100, Loss: 11576899.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 33/100, Loss: 11582442.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 34/100, Loss: 11580326.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 35/100, Loss: 11604321.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 36/100, Loss: 11582190.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 37/100, Loss: 11579726.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 38/100, Loss: 11578983.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 39/100, Loss: 11582582.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 40/100, Loss: 11575624.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 41/100, Loss: 11577919.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 42/100, Loss: 11577235.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 43/100, Loss: 11577315.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 44/100, Loss: 11587162.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 45/100, Loss: 11580098.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 46/100, Loss: 11580380.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 47/100, Loss: 11579371.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 48/100, Loss: 11576281.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 49/100, Loss: 11582315.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 50/100, Loss: 11577541.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 51/100, Loss: 11578717.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 52/100, Loss: 11574994.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 53/100, Loss: 11585852.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 54/100, Loss: 11577716.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 55/100, Loss: 11579091.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 56/100, Loss: 11583175.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 57/100, Loss: 11583076.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 58/100, Loss: 11579125.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 59/100, Loss: 11580440.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 60/100, Loss: 11580309.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 61/100, Loss: 11578418.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 62/100, Loss: 11583381.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 63/100, Loss: 11576811.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 64/100, Loss: 11577366.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 65/100, Loss: 11576685.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 66/100, Loss: 11585318.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 67/100, Loss: 11581173.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 68/100, Loss: 11577296.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 69/100, Loss: 11578008.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 70/100, Loss: 11578152.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 71/100, Loss: 11576322.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 72/100, Loss: 11578028.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 73/100, Loss: 11577013.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 74/100, Loss: 11580463.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 75/100, Loss: 11578664.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 76/100, Loss: 11577488.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 77/100, Loss: 11577316.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 78/100, Loss: 11575915.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 79/100, Loss: 11577702.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 80/100, Loss: 11579213.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 81/100, Loss: 11578334.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 82/100, Loss: 11582034.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 83/100, Loss: 11574924.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 84/100, Loss: 11578942.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 85/100, Loss: 11581046.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 86/100, Loss: 11578719.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 87/100, Loss: 11582405.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 88/100, Loss: 11581394.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 89/100, Loss: 11577196.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 90/100, Loss: 11576509.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 91/100, Loss: 11583660.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 92/100, Loss: 11582070.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 93/100, Loss: 11582188.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 94/100, Loss: 11582892.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 95/100, Loss: 11580205.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 96/100, Loss: 11582701.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 97/100, Loss: 11585284.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 98/100, Loss: 11593831.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 99/100, Loss: 11582459.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 100/100, Loss: 11590778.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "18/18 [==============================] - 1s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Load the input and output data from the pandas DataFrame\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like', 'Pressure', 'External Humidity', 'Dew Point', 'Clouds', 'Wind Speed']]\n",
    "\n",
    "# Convert the pandas DataFrame to numpy arrays with float32 data type\n",
    "X = X.astype('float32').to_numpy()\n",
    "y = y.astype('float32').to_numpy()\n",
    "\n",
    "# Define the model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(128, return_sequences=True, input_shape=(None, 6)))\n",
    "model.add(layers.LSTM(128))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(3))  # Output layer with 3 units for the 3 output columns\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data for the RNN\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_y, test_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and compute the loss\n",
    "    history = model.fit(train_X, train_y, batch_size=32, epochs=1, verbose=0)\n",
    "\n",
    "    # Compute training and testing accuracy\n",
    "    train_loss, train_accuracy = model.evaluate(train_X, train_y, verbose=0)\n",
    "    test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=0)\n",
    "\n",
    "    # Print the loss and accuracy after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {history.history['loss'][0]}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Evaluation\n",
    "predictions = model.predict(test_X)\n",
    "# Perform further evaluation or analysis on the predictions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RESNET</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 12964691.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 2/100, Loss: 11614191.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 3/100, Loss: 11596519.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 4/100, Loss: 11618454.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 5/100, Loss: 11592729.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 6/100, Loss: 11594659.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 7/100, Loss: 11594632.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 8/100, Loss: 11562349.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 9/100, Loss: 11606548.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 10/100, Loss: 11599482.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 11/100, Loss: 11554015.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 12/100, Loss: 11547938.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 13/100, Loss: 11548827.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 14/100, Loss: 11555180.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 15/100, Loss: 11534903.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 16/100, Loss: 11519606.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 17/100, Loss: 11525971.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 18/100, Loss: 11494359.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 19/100, Loss: 11507347.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 20/100, Loss: 11522440.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 21/100, Loss: 11459189.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 22/100, Loss: 11431126.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 23/100, Loss: 11443822.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 24/100, Loss: 11409779.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 25/100, Loss: 11329102.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 26/100, Loss: 11276990.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 27/100, Loss: 11347687.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 28/100, Loss: 11248916.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 29/100, Loss: 11216911.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 30/100, Loss: 11084373.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 31/100, Loss: 11049157.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 32/100, Loss: 11025623.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 33/100, Loss: 10936040.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 34/100, Loss: 10895277.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 35/100, Loss: 10765340.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 36/100, Loss: 10682528.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 37/100, Loss: 10684455.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 38/100, Loss: 10576498.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 39/100, Loss: 10586081.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 40/100, Loss: 10570856.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 41/100, Loss: 10570002.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 42/100, Loss: 10659633.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 43/100, Loss: 10701353.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 44/100, Loss: 10675322.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 45/100, Loss: 10714399.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 46/100, Loss: 10569799.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 47/100, Loss: 10518749.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 48/100, Loss: 10571478.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 49/100, Loss: 10568057.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 50/100, Loss: 10558008.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 51/100, Loss: 10516335.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 52/100, Loss: 10561763.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 53/100, Loss: 10597044.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 54/100, Loss: 10513442.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 55/100, Loss: 10539211.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 56/100, Loss: 10520540.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 57/100, Loss: 10477973.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 58/100, Loss: 10497140.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 59/100, Loss: 10505745.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 60/100, Loss: 10488977.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 61/100, Loss: 10532662.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 62/100, Loss: 10583478.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 63/100, Loss: 10499779.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 64/100, Loss: 10452589.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 65/100, Loss: 10411188.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 66/100, Loss: 10395244.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 67/100, Loss: 10508909.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 68/100, Loss: 10404526.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 69/100, Loss: 10383682.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 70/100, Loss: 10369693.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 71/100, Loss: 10377748.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 72/100, Loss: 10309914.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.49384886026382446\n",
      "Epoch 73/100, Loss: 10320621.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4920913875102997\n",
      "Epoch 74/100, Loss: 10374085.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 75/100, Loss: 10345422.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 76/100, Loss: 10236129.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 77/100, Loss: 10181094.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 78/100, Loss: 10198314.0, Train Accuracy: 0.46481969952583313, Test Accuracy: 0.502636194229126\n",
      "Epoch 79/100, Loss: 10162863.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 80/100, Loss: 10118577.0, Train Accuracy: 0.4929639399051666, Test Accuracy: 0.5377855896949768\n",
      "Epoch 81/100, Loss: 10199372.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.49384886026382446\n",
      "Epoch 82/100, Loss: 10183991.0, Train Accuracy: 0.4621811807155609, Test Accuracy: 0.502636194229126\n",
      "Epoch 83/100, Loss: 10123186.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 84/100, Loss: 10099079.0, Train Accuracy: 0.4656991958618164, Test Accuracy: 0.5061511397361755\n",
      "Epoch 85/100, Loss: 10035010.0, Train Accuracy: 0.4595426619052887, Test Accuracy: 0.5008787512779236\n",
      "Epoch 86/100, Loss: 10121313.0, Train Accuracy: 0.4643799364566803, Test Accuracy: 0.5079085826873779\n",
      "Epoch 87/100, Loss: 9980896.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.49384886026382446\n",
      "Epoch 88/100, Loss: 10027532.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 89/100, Loss: 9970647.0, Train Accuracy: 0.5131925940513611, Test Accuracy: 0.5500878691673279\n",
      "Epoch 90/100, Loss: 9920365.0, Train Accuracy: 0.45778363943099976, Test Accuracy: 0.502636194229126\n",
      "Epoch 91/100, Loss: 9911888.0, Train Accuracy: 0.509674608707428, Test Accuracy: 0.5465729236602783\n",
      "Epoch 92/100, Loss: 10006369.0, Train Accuracy: 0.5241864323616028, Test Accuracy: 0.551845371723175\n",
      "Epoch 93/100, Loss: 9906396.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 94/100, Loss: 9917374.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.49384886026382446\n",
      "Epoch 95/100, Loss: 9850232.0, Train Accuracy: 0.47229552268981934, Test Accuracy: 0.5114235281944275\n",
      "Epoch 96/100, Loss: 9805384.0, Train Accuracy: 0.45998239517211914, Test Accuracy: 0.5043936967849731\n",
      "Epoch 97/100, Loss: 9913530.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4920913875102997\n",
      "Epoch 98/100, Loss: 9850660.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.49384886026382446\n",
      "Epoch 99/100, Loss: 9792935.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 100/100, Loss: 10031964.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "18/18 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Load the input and output data from the pandas DataFrame\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like', 'Pressure', 'External Humidity', 'Dew Point', 'Clouds', 'Wind Speed']]\n",
    "\n",
    "# Convert the pandas DataFrame to numpy arrays with float32 data type\n",
    "X = X.astype('float32').to_numpy()\n",
    "y = y.astype('float32').to_numpy()\n",
    "\n",
    "# Preprocess the input data (optional, depending on your specific use case)\n",
    "# ...\n",
    "\n",
    "# Modify the ResNet architecture for regression\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom regression layers on top of the base model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Input(shape=(6,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(2048, activation='relu'))\n",
    "model.add(layers.Reshape((4, 4, 128)))\n",
    "# model.add(base_model)\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(layers.Dense(3))  # Output layer with 3 units for the 3 output columns\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_y, test_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and compute the loss\n",
    "    history = model.fit(train_X, train_y, batch_size=32, epochs=1, verbose=0)\n",
    "\n",
    "    # Compute training and testing accuracy\n",
    "    train_loss, train_accuracy = model.evaluate(train_X, train_y, verbose=0)\n",
    "    test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=0)\n",
    "\n",
    "    # Print the loss and accuracy after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {history.history['loss'][0]}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Evaluation\n",
    "predictions = model.predict(test_X)\n",
    "# Perform further evaluation or analysis on the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17/72 [======>.......................] - ETA: 8:10 - loss: 19098368.0000 - accuracy: 0.4320"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m regression_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m,metrics\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39m# Train the regression model\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m history\u001b[39m=\u001b[39mregression_model\u001b[39m.\u001b[39;49mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test))\n\u001b[1;32m     61\u001b[0m \u001b[39m# Evaluate the regression model on the training set\u001b[39;00m\n\u001b[1;32m     62\u001b[0m train_loss \u001b[39m=\u001b[39m regression_model\u001b[39m.\u001b[39mevaluate(X_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the input and output data from the pandas DataFrame\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like', 'Pressure', 'External Humidity', 'Dew Point', 'Clouds', 'Wind Speed']]\n",
    "\n",
    "# Convert the pandas DataFrame to numpy arrays with float32 data type\n",
    "X = X.astype('float32').to_numpy()\n",
    "y = y.astype('float32').to_numpy()\n",
    "\n",
    "def dense_block(x, blocks, growth_rate):\n",
    "    for _ in range(blocks):\n",
    "        y = layers.BatchNormalization()(x)\n",
    "        y = layers.Activation('relu')(y)\n",
    "        y = layers.Dense(4 * growth_rate)(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.Activation('relu')(y)\n",
    "        y = layers.Dense(growth_rate)(y)\n",
    "        x = layers.Concatenate()([x, y])\n",
    "    return x\n",
    "\n",
    "def transition_block(x, reduction):\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dense(int(tf.keras.backend.int_shape(x)[-1] * reduction))(x)\n",
    "    return x\n",
    "\n",
    "def build_regression_model(input_shape, blocks_per_layer, growth_rate, reduction, output_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Dense(64)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    for blocks in blocks_per_layer:\n",
    "        x = dense_block(x, blocks, growth_rate)\n",
    "        x = transition_block(x, reduction)\n",
    "    \n",
    "    # x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dense(output_shape, activation='linear')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Create an instance of the regression model\n",
    "input_shape = (6,)\n",
    "blocks_per_layer = [6, 12, 24, 16]\n",
    "growth_rate = 32\n",
    "reduction = 0.5\n",
    "output_shape = 3\n",
    "\n",
    "regression_model = build_regression_model(input_shape, blocks_per_layer, growth_rate, reduction, output_shape)\n",
    "# Compile the regression model\n",
    "regression_model.compile(optimizer='adam', loss='mean_squared_error',metrics='accuracy')\n",
    "\n",
    "# Train the regression model\n",
    "history=regression_model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the regression model on the training set\n",
    "train_loss = regression_model.evaluate(X_train, y_train)\n",
    "\n",
    "# Evaluate the regression model on the testing set\n",
    "test_loss = regression_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the training and testing accuracy\n",
    "train_accuracy = 1 - train_loss\n",
    "test_accuracy = 1 - test_loss\n",
    "\n",
    "print(\"Training Accuracy: {:.2%}\".format(train_accuracy))\n",
    "print(\"Testing Accuracy: {:.2%}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the input and output data from the pandas DataFrame\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like', 'Pressure', 'External Humidity', 'Dew Point', 'Clouds', 'Wind Speed']]\n",
    "\n",
    "# Convert the pandas DataFrame to numpy arrays with float32 data type\n",
    "X = X.astype('float32').to_numpy()\n",
    "y = y.astype('float32').to_numpy()\n",
    "\n",
    "def dense_block(x, blocks, growth_rate):\n",
    "    for _ in range(blocks):\n",
    "        y = layers.BatchNormalization()(x)\n",
    "        y = layers.Activation('relu')(y)\n",
    "        y = layers.Dense(4 * growth_rate)(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.Activation('relu')(y)\n",
    "        y = layers.Dense(growth_rate)(y)\n",
    "        x = layers.Concatenate()([x, y])\n",
    "    return x\n",
    "\n",
    "def transition_block(x, reduction):\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dense(int(tf.keras.backend.int_shape(x)[-1] * reduction))(x)\n",
    "    return x\n",
    "\n",
    "def build_regression_model(input_shape, blocks_per_layer, growth_rate, reduction, output_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Reshape((input_shape[0], 1))(inputs)  # Reshape the input to add a channel dimension\n",
    "    x = layers.Dense(64)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    for blocks in blocks_per_layer:\n",
    "        x = dense_block(x, blocks, growth_rate)\n",
    "        x = transition_block(x, reduction)\n",
    "    \n",
    "    # x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dense(output_shape, activation='linear')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Create an instance of the regression model\n",
    "input_shape = (6,)\n",
    "blocks_per_layer = [6, 12, 24, 16]\n",
    "growth_rate = 32\n",
    "reduction = 0.5\n",
    "output_shape = 3\n",
    "\n",
    "regression_model = build_regression_model(input_shape, blocks_per_layer, growth_rate, reduction, output_shape)\n",
    "# Compile the regression model\n",
    "regression_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the regression model\n",
    "regression_model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the regression model on the training set\n",
    "train_loss = regression_model.evaluateX_train, y_train)\n",
    "\n",
    "# Evaluate the regression model on the testing set\n",
    "test_loss = regression_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the training and testing accuracy\n",
    "train_accuracy = 1 - train_loss\n",
    "test_accuracy = 1 - test_loss\n",
    "\n",
    "print(\"Training Accuracy: {:.2%}\".format(train_accuracy))\n",
    "print(\"Testing Accuracy: {:.2%}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "ed69033dcf95bf03c49614e5fe4e96f21774bed12a1dae221247a7de4d30fa71"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
